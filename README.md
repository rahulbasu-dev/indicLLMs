# indicLLMs ğŸ‡®ğŸ‡³ Culturally-Grounded QA for Meitei (Manipuri)
**Fine-Tuning IndicBERT on Hybrid Datasets for Low-Resource QA**

---

### ğŸ§  Overview

This repository contains all artifacts from the **CS224U Final Project** titled:  
**"Culturally-Aligned QA for Meitei: Fine-Tuning IndicBERT with Hybrid Datasets"**  
by **Rahul Basu**, Founder â€“ humaNLP Mindful AI Solutions LLP.

The project addresses the scarcity of NLP resources for **Meitei (Manipuri)**, a low-resource language spoken in Northeast India, by creating a culturally-grounded extractive Question Answering (QA) system using a fine-tuned **IndicBERT** model.  

---

### ğŸ¯ Objectives

- Build a lightweight, semantically aligned QA system for Meitei
- Fine-tune IndicBERT using a hybrid dataset of:
  - 500 culturally specific QA pairs (generated synthetically)
  - 200 translated SQuAD QA pairs
- Evaluate performance using **BERTScore**, **ROUGE-L**, and **BLEU**
- Release a reproducible pipeline and open dataset under an open license

---

### ğŸ“‚ Repository Structure
<pre> repo-root/ â”œâ”€â”€ <b>data/</b> â”‚ â”œâ”€â”€ meitei_cqa.jsonl # 500 culturally grounded QA pairs â”‚ â”œâ”€â”€ translated_squad_meitei.jsonl # 200 Meitei-translated SQuAD QA pairs â”‚ â””â”€â”€ preprocessed/ # Preprocessed and tokenized versions â”‚ â”œâ”€â”€ <b>models/</b> â”‚ â””â”€â”€ fine_tuned_indicbert/ # Hugging Face model checkpoint directory â”‚ â”œâ”€â”€ <b>scripts/</b> â”‚ â”œâ”€â”€ preprocess.py # Data cleaning, Romanization â”‚ â”œâ”€â”€ train.py # Hugging Face Trainer API â”‚ â”œâ”€â”€ evaluate.py # BERTScore, BLEU, ROUGE-L â”‚ â”œâ”€â”€ <b>notebooks/</b> â”‚ â””â”€â”€ QA_pipeline_walkthrough.ipynb # Step-by-step training and inference â”‚ â”œâ”€â”€ <b>results/</b> â”‚ â””â”€â”€ evaluation_metrics.json # Final model evaluation scores â”‚ â””â”€â”€ README.md # Project overview and documentation </pre>

---

### ğŸ§ª Experimental Setup

- **Model**: `ai4bharat/IndicBERT` with QA head (`AutoModelForQuestionAnswering`)
- **Training Data**: 700 QA pairs split into train (80%), val (10%), test (10%)
- **Script Used**: Romanized Meitei (due to tokenizer limitations in IndicBERT)
- **Training Platform**: Single NVIDIA T4 (16 GB)
- **Framework**: Hugging Face Transformers + Datasets

---

### ğŸ§® Metrics & Performance

| Model Variant               | BERTScore (F1) | ROUGE-L | BLEU |
|----------------------------|----------------|----------|------|
| IndicBERT (Fine-Tuned)     | **0.82**       | 0.17     | 0.00 |
| IndicBERT (Zero-Shot)      | 0.62           | 0.11     | 0.00 |
| English QA + Backtrans     | 0.76           | 0.15     | 0.00 |
| Random Span Baseline       | 0.40           | 0.06     | 0.00 |

*Note: Semantic metrics like BERTScore are more reliable in morphologically rich languages like Meitei.*

---

### ğŸ§© Key Features

- âœ… **Cultural Alignment**: QA pairs based on festivals, governance, oral traditions, and rituals
- âœ… **Tokenizer Adaptation**: Custom Romanization pipeline
- âœ… **Open Dataset**: JSONL format, released under open license
- âœ… **Reproducible Codebase**: Easily extensible to other Indic languages

---

### ğŸ”¬ Research Contributions

- First reproducible QA pipeline for Meitei using LLMs
- Evaluation of fine-tuned IndicBERT vs zero-shot and translation baselines
- Released dataset + training scripts for the research community
- Analysis of overfitting, tokenization errors, and metric gaps

---

### ğŸ· License

This repository is released under the **MIT License** for code and **CC BY 4.0** for datasets.

---

### ğŸ‘¤ Author

**Rahul Basu**  
ğŸ“§ rahulbasuai@gmail.com  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/basu-rahul)

---

### ğŸ™Œ Acknowledgements

- Stanford CS224U Teaching Team
- AI4Bharat for `indictrans2` and `IndicBERT`
- Community contributors working on Indic NLP, cultural alignment, and low-resource language access

---

### ğŸ”­ Future Work

- Add more culturally validated QA pairs in Meitei Mayek
- Extend to other low-resource languages like Bhojpuri and Konkani
- Add adapter-based fine-tuning and multilingual alignment

